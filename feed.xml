<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://chen-hao-chao.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://chen-hao-chao.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-18T16:47:16+00:00</updated><id>https://chen-hao-chao.github.io/feed.xml</id><title type="html">blank</title><subtitle>The personal website of Chen-Hao Chao. </subtitle><entry><title type="html">Beyond Masked and Unmasked: Discrete Diffusion Models via Partial Masking</title><link href="https://chen-hao-chao.github.io/blog/2025/beyond-masked-and-unmasked-discrete-diffusion-models-via-partial-masking/" rel="alternate" type="text/html" title="Beyond Masked and Unmasked: Discrete Diffusion Models via Partial Masking"/><published>2025-06-19T00:00:00+00:00</published><updated>2025-06-19T00:00:00+00:00</updated><id>https://chen-hao-chao.github.io/blog/2025/beyond-masked-and-unmasked-discrete-diffusion-models-via-partial-masking</id><content type="html" xml:base="https://chen-hao-chao.github.io/blog/2025/beyond-masked-and-unmasked-discrete-diffusion-models-via-partial-masking/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[This blog post offers an introduction to MDM-Prime, a generalized masked diffusion model (MDM) that enables partially unmasked tokens during sampling. We begin with an review of MDMs and their limitations. Then, we explore a Partial masking scheme (Prime) that introduces intermediate token states between masked and unmasked representations. Finally, we present experimental results to demonstrate the effectiveness of MDM-Prime.]]></summary></entry><entry><title type="html">Maximum Entropy Reinforcement Learning via Energy-Based Normalizing Flow</title><link href="https://chen-hao-chao.github.io/blog/2024/maximum-entropy-reinforcement-learning-via-energy-based-normalizing-flow/" rel="alternate" type="text/html" title="Maximum Entropy Reinforcement Learning via Energy-Based Normalizing Flow"/><published>2024-10-23T00:00:00+00:00</published><updated>2024-10-23T00:00:00+00:00</updated><id>https://chen-hao-chao.github.io/blog/2024/maximum-entropy-reinforcement-learning-via-energy-based-normalizing-flow</id><content type="html" xml:base="https://chen-hao-chao.github.io/blog/2024/maximum-entropy-reinforcement-learning-via-energy-based-normalizing-flow/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[This blog post offers an introduction to our proposed MEow algorithm. We begin with an review of MaxEnt RL and EBFlow. Then, we explore the connections between these models by introducing MEow. Finally, we present experimental results to demonstrate the effectiveness of the proposed method.]]></summary></entry><entry><title type="html">Training Energy-Based Normalizing Flow with Score-Matching Objectives</title><link href="https://chen-hao-chao.github.io/blog/2023/training-energy-based-normalizing-flow-with-score-matching-objectives/" rel="alternate" type="text/html" title="Training Energy-Based Normalizing Flow with Score-Matching Objectives"/><published>2023-05-24T00:00:00+00:00</published><updated>2023-05-24T00:00:00+00:00</updated><id>https://chen-hao-chao.github.io/blog/2023/training-energy-based-normalizing-flow-with-score-matching-objectives</id><content type="html" xml:base="https://chen-hao-chao.github.io/blog/2023/training-energy-based-normalizing-flow-with-score-matching-objectives/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[This blog post offers an introduction to our proposed EBFlow modeling method. First, we begin with an overview of flow-based and energy-based models. Then, we explore the connections between these models by introducing EBFlow. Next, we present experimental results to demonstrate the effectiveness of the proposed method. Finally, we discuss several implications of the EBFlow formula.]]></summary></entry><entry><title type="html">On Investigating the Conservative Property of Score-Based Generative Models</title><link href="https://chen-hao-chao.github.io/blog/2022/on-investigating-the-conservative-property-of-score-based-generative-models/" rel="alternate" type="text/html" title="On Investigating the Conservative Property of Score-Based Generative Models"/><published>2022-09-26T00:00:00+00:00</published><updated>2022-09-26T00:00:00+00:00</updated><id>https://chen-hao-chao.github.io/blog/2022/on-investigating-the-conservative-property-of-score-based-generative-models</id><content type="html" xml:base="https://chen-hao-chao.github.io/blog/2022/on-investigating-the-conservative-property-of-score-based-generative-models/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[This blog post provides an introduction to our proposed QCSBM modeling method. We first start with motivational examples that examine the influence of the conservative property of score-based models. Next, we outline a training pipeline designed to backpropagate the conservativeness through the model. Finally, we present experimental results to demonstrate the effectiveness of our method.]]></summary></entry><entry><title type="html">Denoising Likelihood Score Matching for Conditional Score-based Data Generation</title><link href="https://chen-hao-chao.github.io/blog/2022/denoising-likelihood-score-matching-for-conditional-score-based-data-generation/" rel="alternate" type="text/html" title="Denoising Likelihood Score Matching for Conditional Score-based Data Generation"/><published>2022-05-27T00:00:00+00:00</published><updated>2022-05-27T00:00:00+00:00</updated><id>https://chen-hao-chao.github.io/blog/2022/denoising-likelihood-score-matching-for-conditional-score-based-data-generation</id><content type="html" xml:base="https://chen-hao-chao.github.io/blog/2022/denoising-likelihood-score-matching-for-conditional-score-based-data-generation/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[This blog post provides an introduction to our proposed DLSM training method. First, we start with an introduction of the denoising score-matching (DSM) method. Then, we discuss the limitations of the current conditional score-based generation methods. Next, we formulate the proposed DLSM loss. Finally, we present experimental results to demonstrate the effectiveness of DLSM.]]></summary></entry><entry><title type="html">Rethinking Ensemble-Distillation for Semantic Segmentation Based Unsupervised Domain Adaptation</title><link href="https://chen-hao-chao.github.io/blog/2021/rethinking-ensemble-distillation-for-semantic-segmentation-based-unsupervised-domain-adaptation/" rel="alternate" type="text/html" title="Rethinking Ensemble-Distillation for Semantic Segmentation Based Unsupervised Domain Adaptation"/><published>2021-04-29T00:00:00+00:00</published><updated>2021-04-29T00:00:00+00:00</updated><id>https://chen-hao-chao.github.io/blog/2021/rethinking-ensemble-distillation-for-semantic-segmentation-based-unsupervised-domain-adaptation</id><content type="html" xml:base="https://chen-hao-chao.github.io/blog/2021/rethinking-ensemble-distillation-for-semantic-segmentation-based-unsupervised-domain-adaptation/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[End-to-end ensemble learning methods often lack flexibility as any modification to the ensemble requires retraining of their frameworks. To address this problem, we propose a flexible ensemble-distillation framework for performing semantic segmentation based UDA, allowing any arbitrary composition of the members in the ensemble while still maintaining its superior performance. To achieve such flexibility, our framework is designed to be robust against the output inconsistency and the performance variation of the members within the ensemble.]]></summary></entry></feed>